{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Recommender system notebook.ipynb",
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "https://github.com/Lebo1024/FNBioscope_Recommender_System/blob/main/Recommender_system_notebook.ipynb",
      "authorship_tag": "ABX9TyODKlt1omQ+be/YJtCuwAzV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lebo1024/FNBioscope_Recommender_System/blob/main/Recommender_system_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJD0Tbq_pAJU"
      },
      "source": [
        "# **Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdGAdDfApPK1"
      },
      "source": [
        "In todayâ€™s technology driven world, recommender systems are socially and economically critical for ensuring that individuals can make appropriate choices surrounding the content they engage with on a daily basis. One application where this is especially true surrounds movie content recommendations, where intelligent algorithms can help viewers find great titles from tens of thousands of options.\r\n",
        "\r\n",
        "This notebook follows the step-by-step process to construct a recommendation algorithm based on content or collaborative filtering, capable of accurately predicting how a user will rate a movie they have not yet viewed based on their historical preferences.\r\n",
        "\r\n",
        "Providing an accurate and robust solution to this challenge has immense economic potential, with users of the system being exposed to content they would like to view or purchase - generating revenue and platform affinity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sWj8n--iSZv"
      },
      "source": [
        "pip install surprise"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGc63sz0iCiC"
      },
      "source": [
        "# data analysis libraries\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "# Kaggle requirements\r\n",
        "import os\r\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\r\n",
        "    for filename in filenames:\r\n",
        "        print(os.path.join(dirname, filename))\r\n",
        "        \r\n",
        "\r\n",
        "# visualisation libraries\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "from numpy.random import RandomState\r\n",
        "\r\n",
        "\r\n",
        "#word cloud\r\n",
        "%matplotlib inline\r\n",
        "import wordcloud\r\n",
        "\r\n",
        "from wordcloud import WordCloud, STOPWORDS\r\n",
        "%matplotlib inline\r\n",
        "sns.set()\r\n",
        "\r\n",
        "# visualisation libraries\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "from plotly.offline import init_notebook_mode, plot, iplot\r\n",
        "import plotly.graph_objs as go\r\n",
        "init_notebook_mode(connected=True)\r\n",
        "import plotly.express as px\r\n",
        "import plotly.figure_factory as ff\r\n",
        "import plotly.graph_objects as go\r\n",
        "import plotly.graph_objs as go\r\n",
        "import plotly.offline as pyo\r\n",
        "\r\n",
        "\r\n",
        "# ML Models\r\n",
        "from surprise import Reader\r\n",
        "from surprise import Dataset\r\n",
        "from surprise.model_selection import cross_validate\r\n",
        "from surprise import NormalPredictor\r\n",
        "from surprise import KNNBasic\r\n",
        "from surprise import KNNWithMeans\r\n",
        "from surprise import KNNWithZScore\r\n",
        "from surprise import KNNBaseline\r\n",
        "from surprise import SVD\r\n",
        "from surprise import BaselineOnly\r\n",
        "from surprise import SVDpp\r\n",
        "from surprise import NMF\r\n",
        "from surprise import SlopeOne\r\n",
        "from surprise import CoClustering\r\n",
        "from surprise.accuracy import rmse\r\n",
        "from surprise import accuracy\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "from sklearn.metrics.pairwise import cosine_similarity\r\n",
        "\r\n",
        "# ML Pre processing\r\n",
        "from surprise.model_selection import train_test_split\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "\r\n",
        "# Hyperparameter tuning\r\n",
        "from surprise.model_selection import GridSearchCV\r\n",
        "\r\n",
        "# High performance hyperparameter tuning\r\n",
        "#from tune_sklearn import TuneSearchCV\r\n",
        "#import warnings\r\n",
        "#warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUXcIe3vqIS9"
      },
      "source": [
        "# **Data Imports**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpfK9sYNqr96"
      },
      "source": [
        "The Expected data sets are as follows:\r\n",
        "\r\n",
        " \r\n",
        "\r\n",
        "*   genome_scores.csv - a score mapping the strength between movies and tag-related properties.\r\n",
        "\r\n",
        "\r\n",
        "*   genome_tags.csv - user assigned tags for genome-related scores\r\n",
        "*   imdb_data.csv - Additional movie metadata scraped from IMDB using the links.csv file\r\n",
        "\r\n",
        "\r\n",
        "*   links.csv - File providing a mapping between a MovieLens ID and associated IMDB and TMDB IDs.\r\n",
        "*sample_submission.csv - Sample of the submission format for the hackathon.\r\n",
        "tags.csv - User assigned for the movies within the dataset.\r\n",
        "*test.csv - The test split of the dataset. Contains user and movie IDs with no rating data.\r\n",
        "*train.csv - The training split of the dataset. Contains user and movie IDs with associated rating data.\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        ".\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7j_U6Wn3jDjM"
      },
      "source": [
        "test_df = pd.read_csv('/content/drive/MyDrive/edsa-recommender-system-predict/test.csv')\r\n",
        "movies = pd.read_csv('/content/drive/MyDrive/edsa-recommender-system-predict/movies.csv')\r\n",
        "train = pd.read_csv(\"/content/drive/MyDrive/edsa-recommender-system-predict/train.csv\")\r\n",
        "imdb = pd.read_csv('/content/drive/MyDrive/edsa-recommender-system-predict/imdb_data.csv')\r\n",
        "gtags = pd.read_csv(\"/content/drive/MyDrive/edsa-recommender-system-predict/genome_tags.csv\")\r\n",
        "gscores = pd.read_csv(\"/content/drive/MyDrive/edsa-recommender-system-predict/genome_scores.csv\")\r\n",
        "tags = pd.read_csv(\"/content/drive/MyDrive/edsa-recommender-system-predict/tags.csv\")\r\n",
        "links = pd.read_csv(\"/content/drive/MyDrive/edsa-recommender-system-predict/links.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5_alS0TqZoX"
      },
      "source": [
        "# **Basic Data Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdvSGUWEqFOV"
      },
      "source": [
        "# Display top 5 rows of dataframe\r\n",
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtpOdr3Ft7lf"
      },
      "source": [
        "#Viewing movies data\r\n",
        "movies.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euxC9f1LuBaP"
      },
      "source": [
        "#Viewing imdb dataframe\r\n",
        "\r\n",
        "imdb.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IRTWnDko_Py"
      },
      "source": [
        "#Viewing Genrome tags\r\n",
        "gtags.head()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JniXE83uJ7r"
      },
      "source": [
        "#Viewing scores\r\n",
        "gscores.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-c9Qam0uPhl"
      },
      "source": [
        "#viewing tags\r\n",
        "tags.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIzdunA0unlH"
      },
      "source": [
        "#view links\r\n",
        "links.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZX6gWTLFuych"
      },
      "source": [
        "# **Data Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJqm8CRPu5ET"
      },
      "source": [
        "Preparing raw data:\r\n",
        "\r\n",
        "We will first prepare this raw data to make it suitable for our machine learning model. This is a very crucial step while for creating a machine learning model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-YW6gBfvDp0"
      },
      "source": [
        "### **Checking for missing values column wise**\r\n",
        "\r\n",
        "**Handling Missing Data:**\r\n",
        "\r\n",
        "In our dataset, there may be some missing values. We cannot train our model with a dataset that contains missing values. So we have to check if our dataset has missing values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdWUFP7eurN0"
      },
      "source": [
        "#check for missing values\r\n",
        "train.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MB-ZR-evWP-"
      },
      "source": [
        "## **Checking for duplicates records**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCVKxJpzvZ-v"
      },
      "source": [
        "# check duplicates\r\n",
        "dup_bool = train.duplicated(['userId', 'movieId', 'rating'])\r\n",
        "\r\n",
        "# display duplicates\r\n",
        "print(\"Number of duplicate records:\", sum(dup_bool))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPmuM5nbwiWj"
      },
      "source": [
        "## **Creating a copy**\r\n",
        "\r\n",
        "We will rename our train data as df and look at the top 5 records in the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uB_llm8ovozk"
      },
      "source": [
        "# Create a copy\r\n",
        "df = train.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3d9bD4QNw1Zr"
      },
      "source": [
        "# Create a copy of the train data\r\n",
        "df_train = train.copy()\r\n",
        "\r\n",
        "# Display top 5 records\r\n",
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UV9OJcM-w8gA"
      },
      "source": [
        "## **Evaluating Length of Unique Values**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CA48tkrLxAyQ"
      },
      "source": [
        "# Find the length of the unique use\r\n",
        "len(df_train['userId'].unique()), len(df_train['movieId'].unique())\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDn8F7KaxRbQ"
      },
      "source": [
        "# View movies\r\n",
        "movies.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJbZbOJfxKfG"
      },
      "source": [
        "# View unique values of movies\r\n",
        "len(movies['movieId'].unique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZZ3jF6Mxs-M"
      },
      "source": [
        "## **Joining Datasets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVXUdDRCx04k"
      },
      "source": [
        "# Merge the ratings and movies\r\n",
        "df_merge1 = train.merge(movies, on='movieId')\r\n",
        "# View the first 5 rows\r\n",
        "df_merge1.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMOExGAAyCkk"
      },
      "source": [
        "# Merging the dataset with that of the imbd\r\n",
        "df_merge2 = train.merge(imdb, on=\"movieId\")\r\n",
        "# View first 5 rows\r\n",
        "df_merge2.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwzGufs3y0BU"
      },
      "source": [
        "# Merging the merge data earlier on with the df_imbd\r\n",
        "df_merge3 = df_merge1.merge(imdb, on=\"movieId\" )\r\n",
        "# View first 5 rows\r\n",
        "df_merge3.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXVUoL3RzKkI"
      },
      "source": [
        "# Check the null values of the data that has just been merged.\r\n",
        "df_merge3.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-ro7QrW0xK7"
      },
      "source": [
        "# **Exploratory Data Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrGdejNv03LG"
      },
      "source": [
        "## **Missing Data and Data Types**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_8SPuC81C5M"
      },
      "source": [
        "In order to facilitate the identification of missing data and data types, a function, print_dtypes_missing, is defined below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWCJj5Wl01Vk"
      },
      "source": [
        "def print_dtypes_null(df):\r\n",
        "    \r\n",
        "    \"\"\"\r\n",
        "    This function takes a dataframe as input and prints out the\r\n",
        "    datatypes and null values datatypes of the dataframe\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    # print data types\r\n",
        "    print('Data type')\r\n",
        "    print(df.info(),'\\n======================')\r\n",
        "    \r\n",
        "    \r\n",
        "    # get number of null values\r\n",
        "    total = df.isnull().sum().sort_values(ascending=False)\r\n",
        "    \r\n",
        "    # get percentage null values\r\n",
        "    percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)*100\r\n",
        "    \r\n",
        "    # create dataframe\r\n",
        "    print('Missing Values')\r\n",
        "    print(pd.concat([total, percent], axis=1, keys=['Total Number Missing', 'Percent Missing']),'\\n======================')\r\n",
        "    \r\n",
        "    # print original dataframe for ease of reading\r\n",
        "    print('Dataset')\r\n",
        "    print(df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0ToWkxg1M0t"
      },
      "source": [
        "print_dtypes_null(train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "preMvUrR1OOH"
      },
      "source": [
        "print_dtypes_null(test_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4lvpVf51Ukb"
      },
      "source": [
        "print_dtypes_null(gscores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ScMtgS01gpA"
      },
      "source": [
        "print_dtypes_null(gtags)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShMC1qsw1qbt"
      },
      "source": [
        "print_dtypes_null(movies)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUa6C_zd1-zh"
      },
      "source": [
        "print_dtypes_null(imdb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOfwygSl2kva"
      },
      "source": [
        "imdb_data_df consists of numerical data, float64 and has no 5 columns with missing data ranging from 36% to for director to 71% budget"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXeLoAc92G6m"
      },
      "source": [
        "print_dtypes_null(links)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txQoOZx73t4U"
      },
      "source": [
        "links_df consists of numerical data, int64 and float64 and has 1 column, tmdbId with 17% missing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4emtI0K2O5S"
      },
      "source": [
        "print_dtypes_null(tags)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAEaqske3453"
      },
      "source": [
        "tags_df consists of numerical data, int64 , and non-numeric data, object ,and has less than 1% missing values for tag column"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5BpRnk036Jy"
      },
      "source": [
        "**Conclusion:**\r\n",
        "\r\n",
        "1.) From the assessment we see that our dataset consists of a combination of numeric and non-numeric data types.\r\n",
        "\r\n",
        "2.) The imdb_data_df dataset is has 36% - 71% missing data across all the columns. This datatset will therefore not be considered going forward in this excercise. In a different context however, the links_df dataset would be used to source the missing data from a supplementary dataset. The links_df dataset will also not be considered going forward."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIV0tZgl4KAN"
      },
      "source": [
        "# remove data that will not be considered\r\n",
        "del imdb\r\n",
        "del links"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BA6NTph98EVj"
      },
      "source": [
        "# **Exploratory Data Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSJFG-Wi5VIE"
      },
      "source": [
        "def make_histogram(df, col):\r\n",
        "\r\n",
        "\r\n",
        "    # Plot the histogram with default number of bins; label your axes\r\n",
        "    _ = plt.hist(df[col])\r\n",
        "    _ = plt.xlabel(col)\r\n",
        "    _ = plt.xticks(rotation=90)\r\n",
        "    _ = plt.ylabel('Frequency')\r\n",
        "    \r\n",
        "    plt.savefig(f'Histogram of {col}.png')\r\n",
        "\r\n",
        "    # Show the plot\r\n",
        "    plt.show()\r\n",
        "\r\n",
        "\r\n",
        "def show_wordcloud(data, col):\r\n",
        "    \r\n",
        "    # define text from data\r\n",
        "    text = ' '.join(data[col].values.astype(str))\r\n",
        "    \r\n",
        "    # generate wordclound\r\n",
        "    wordcloud = WordCloud(max_words=50,\r\n",
        "                          background_color='black',\r\n",
        "                          scale=3,\r\n",
        "                          random_state=4).generate(str(text))\r\n",
        "    \r\n",
        "    # plot wordcloud\r\n",
        "    fig = plt.figure(1, figsize=(15, 15))\r\n",
        "    plt.axis('off')\r\n",
        "        \r\n",
        "    plt.savefig(f'Word cloud of {col}.png')\r\n",
        "    plt.imshow(wordcloud)\r\n",
        "    plt.show()\r\n",
        "\r\n",
        "\r\n",
        "def ecdf(data):\r\n",
        "    \r\n",
        "    \"\"\"Compute ECDF for a one-dimensional array of measurements.\"\"\"\r\n",
        "    \r\n",
        "    # Number of data points: n\r\n",
        "    n = len(data)\r\n",
        "\r\n",
        "    # x-data for the ECDF: x\r\n",
        "    x = np.sort(data)\r\n",
        "\r\n",
        "    # y-data for the ECDF: y\r\n",
        "    y = np.arange(1, n+1) / n\r\n",
        "\r\n",
        "    return x, y\r\n",
        "\r\n",
        "\r\n",
        "def plot_ecdf(df, col):\r\n",
        "    \r\n",
        "    \"\"\"plot ECDF for a column, col, in a dataframe, df.\"\"\"\r\n",
        "    \r\n",
        "    # Compute ECDF \r\n",
        "    x, y = ecdf(df[col])\r\n",
        "    \r\n",
        "    # Generate plot\r\n",
        "    _ = plt.plot(x, y, marker='.', linestyle = 'none')\r\n",
        "    \r\n",
        "    # Label axes\r\n",
        "    plt.ylabel('ECDF')\r\n",
        "    plt.xlabel(f'{col}')\r\n",
        "    \r\n",
        "    \r\n",
        "    plt.savefig(f'ecdf of {col}.png')\r\n",
        "    \r\n",
        "    # display\r\n",
        "    plt.show()\r\n",
        "    \r\n",
        "    \r\n",
        "def plot_category_distribution(data, category, value, plot_type=sns.violinplot):\r\n",
        "    \r\n",
        "    \"\"\"\r\n",
        "    To create a distribution plot. The standard plot type is violing plot.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    # Create bee swarm plot with Seaborn's default settings\r\n",
        "    _ = plot_type(x=category, y=value, data=data)\r\n",
        "\r\n",
        "    # Label Title and axes\r\n",
        "    _ = plt.title(f'distribution of {category} vs {value}')\r\n",
        "    _ = plt.xlabel(category)\r\n",
        "    _ = plt.xticks(rotation=90)\r\n",
        "    _ = plt.ylabel(value)\r\n",
        "    \r\n",
        "    \r\n",
        "    # save the plot\r\n",
        "    plt.savefig(f'distribution of {category} vs {value}.png')\r\n",
        "\r\n",
        "    # Show the plot\r\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLNtmBZY8YnJ"
      },
      "source": [
        "train.describe().T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKTKm7fn8aop"
      },
      "source": [
        "train.nunique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "of3LTRfl9DRb"
      },
      "source": [
        "# remove timestamp\r\n",
        "train.drop('timestamp', axis=1, inplace=True)\r\n",
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wa5iQ6SE9Wvb"
      },
      "source": [
        "There are 10 000 038 records in the train_df dataset. However there are 162 541 usersIDs with 48 213 movies that interacted with them. There are 10 unique ratings that were made and 8 795 101 different times.\r\n",
        "\r\n",
        "It was assumed that people view different movies at different times for reasons that have little or nothing to do with movies they like. For this reason, The timestamp data will not be assessed going forward in this exercise\r\n",
        "\r\n",
        "The rating data was be explored below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gYOrvUP9XpC"
      },
      "source": [
        "# Determining number of rows for each rating value\r\n",
        "rows_rating = train[\"rating\"].value_counts()\r\n",
        "rows_rating_df = pd.DataFrame({\"rating\": rows_rating.index, \"Rows\": rows_rating.values})\r\n",
        "\r\n",
        "# Determining percentage of rows for each rating value\r\n",
        "percentage_rating = round(train[\"rating\"].value_counts(normalize=True) * 100, 2)\r\n",
        "percentage_rating_df = pd.DataFrame(\r\n",
        "    {\"rating\": percentage_rating.index, \"Percentage\": percentage_rating.values}\r\n",
        ")\r\n",
        "\r\n",
        "# Joining row and percentage information\r\n",
        "ratings_distribution_df = pd.merge(\r\n",
        "    rows_rating_df, percentage_rating_df, on=\"rating\", how=\"outer\"\r\n",
        ")\r\n",
        "ratings_distribution_df.set_index(\"rating\", inplace=True)\r\n",
        "ratings_distribution_df.sort_index(axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CAp_Lk2-M3w"
      },
      "source": [
        "In the dataframe above we can see that 4.0 is the most commonly score, with 26.53% of the movies in the dataframe assigned that score.\r\n",
        "\r\n",
        "We visualize the data below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vk5g9q2hAHEz"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "from plotly.offline import init_notebook_mode, iplot, plot\r\n",
        "from plotly.subplots import make_subplots\r\n",
        "import plotly.express as px\r\n",
        "import plotly.figure_factory as ff\r\n",
        "import plotly.graph_objects as go\r\n",
        "import plotly.graph_objs as go\r\n",
        "import plotly.offline as pyo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKI6UlLmANC_"
      },
      "source": [
        "pyo.init_notebook_mode()\r\n",
        "init_notebook_mode(connected=True)\r\n",
        "data = train[\"rating\"].value_counts().sort_index(ascending=False)\r\n",
        "\r\n",
        "# Plot data\r\n",
        "trace = go.Bar(\r\n",
        "    x=data.index,\r\n",
        "    text=[\"{:.1f} %\".format(val) for val in (data.values / train.shape[0] * 100)],\r\n",
        "    textposition=\"auto\",\r\n",
        "    textfont=dict(color=\"#000000\"),\r\n",
        "    y=data.values,\r\n",
        ")\r\n",
        "\r\n",
        "# Create layout\r\n",
        "layout = dict(\r\n",
        "    title=\"Distribution Of {} ratings\".format(train.shape[0]),\r\n",
        "    xaxis=dict(title=\"Rating\"),\r\n",
        "    yaxis=dict(title=\"Count\"),\r\n",
        ")\r\n",
        "\r\n",
        "# Create plot\r\n",
        "fig = go.Figure(data=[trace], layout=layout)\r\n",
        "pyo.iplot(fig)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZTxJP7PBI5d"
      },
      "source": [
        "Interestingly, we see half scores (0.5, 1.5, 2.5, 3.5 and 4.5) are less commonly used than integer score values. We don't know if this is because users prefer to rate movies with integer values or if it's because half scores were introduced after the original scoring system was already in use, leading to a decreased volume in a dataset with ratings from 1995. We quickly attempt to understand this further by investigating which years recorded half-score ratings"
      ]
    }
  ]
}